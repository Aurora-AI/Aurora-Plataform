import asyncio
import aiohttp
from bs4 import BeautifulSoup
import csv
import locale
from datetime import datetime
import re

# Configurar locale para portugu√™s
try:
    locale.setlocale(locale.LC_TIME, "pt_BR.UTF-8")
except:
    try:
        locale.setlocale(locale.LC_TIME, "Portuguese_Brazil.1252")
    except:
        print("‚ö†Ô∏è N√£o foi poss√≠vel configurar locale portugu√™s, usando padr√£o")


def extrair_data_da_url(url):
    """Extrai data da URL da not√≠cia"""
    # Padr√£o: /jornal-do-medico-19-07-2025
    match = re.search(r"jornal-do-medico-(\d{2})-(\d{2})-(\d{4})", url)
    if match:
        dia, mes, ano = match.groups()
        return f"{ano}-{mes}-{dia}"

    # Padr√£o alternativo: buscar qualquer data na URL
    match = re.search(r"(\d{2})-(\d{2})-(\d{4})", url)
    if match:
        dia, mes, ano = match.groups()
        return f"{ano}-{mes}-{dia}"

    # Se n√£o conseguir extrair, usar data atual como fallback
    return datetime.now().strftime("%Y-%m-%d")


def extrair_titulo(soup, url):
    """Extrai t√≠tulo da not√≠cia usando m√∫ltiplas estrat√©gias"""
    # Estrat√©gia 1: Segundo H1 (mais espec√≠fico)
    h1_elements = soup.find_all("h1")
    if len(h1_elements) > 1:
        titulo = h1_elements[1].get_text(strip=True)
        if titulo:
            return titulo

    # Estrat√©gia 2: Primeiro H1 n√£o vazio
    for h1 in h1_elements:
        titulo = h1.get_text(strip=True)
        if titulo:
            return titulo

    # Estrat√©gia 3: Title da p√°gina (removendo " ‚Äì AMB")
    title_tag = soup.find("title")
    if title_tag:
        titulo_completo = title_tag.get_text(strip=True)
        # Remove " ‚Äì AMB" do final
        titulo = titulo_completo.replace(" ‚Äì AMB", "").strip()
        if titulo:
            return titulo

    # Estrat√©gia 4: Tentar extrair do pr√≥prio URL
    if "jornal-do-medico" in url:
        partes = url.split("/")[-2] if url.endswith("/") else url.split("/")[-1]
        # Converter hifens em espa√ßos e capitalizar
        titulo_url = partes.replace("-", " ").replace("jornal do medico", "").strip()
        if titulo_url:
            return titulo_url.title()

    return "T√≠tulo n√£o encontrado"


async def extrair_conteudo_artigo(session, url):
    """Extrai o conte√∫do completo de um artigo"""
    try:
        async with session.get(
            url, timeout=aiohttp.ClientTimeout(total=15)
        ) as response:
            if response.status != 200:
                return None, "T√≠tulo n√£o encontrado"

            html = await response.text()
            soup = BeautifulSoup(html, "html.parser")

            # Extrair t√≠tulo usando nova estrat√©gia
            titulo = extrair_titulo(soup, url)

            # Remover scripts e estilos
            for script in soup(["script", "style"]):
                script.decompose()

            # Buscar o conte√∫do principal
            seletores_conteudo = [
                "article",
                ".post-content",
                ".entry-content",
                ".content",
                "main",
                ".single-post",
            ]

            conteudo = ""
            for seletor in seletores_conteudo:
                elemento = soup.select_one(seletor)
                if elemento:
                    conteudo = elemento.get_text(separator="\n", strip=True)
                    break

            # Se n√£o encontrou com seletores espec√≠ficos, pega o body
            if not conteudo:
                body = soup.find("body")
                if body:
                    conteudo = body.get_text(separator="\n", strip=True)

            # Limpar e formatar o conte√∫do
            linhas = [linha.strip() for linha in conteudo.split("\n") if linha.strip()]
            conteudo_limpo = "\n".join(linhas)

            return conteudo_limpo[:5000], titulo  # Limitar conte√∫do a 5000 chars

    except Exception as e:
        print(f"‚ùå Erro ao processar {url}: {e}")
        return None, "T√≠tulo n√£o encontrado"


async def corrigir_csv_noticias():
    """Corrige o CSV existente com t√≠tulos corretos"""

    print("üîÑ Iniciando corre√ß√£o de t√≠tulos do CSV de not√≠cias da AMB...")

    # Ler CSV existente
    noticias_para_corrigir = []
    with open("noticias_amb_desde_2019.csv", "r", encoding="utf-8") as arquivo:
        leitor = csv.DictReader(arquivo)
        for linha in leitor:
            if linha["titulo"] == "T√≠tulo n√£o encontrado":
                noticias_para_corrigir.append(linha)

    print(f"üìä Total de not√≠cias para corrigir t√≠tulos: {len(noticias_para_corrigir)}")

    # Processar em lotes menores para n√£o sobrecarregar
    tamanho_lote = 50
    noticias_corrigidas = []

    async with aiohttp.ClientSession() as session:
        for i in range(0, len(noticias_para_corrigir), tamanho_lote):
            lote = noticias_para_corrigir[i : i + tamanho_lote]
            print(
                f"üîÑ Processando lote {i//tamanho_lote + 1}/{(len(noticias_para_corrigir) + tamanho_lote - 1)//tamanho_lote}..."
            )

            # Processar lote
            tasks = []
            for noticia in lote:
                tasks.append(extrair_conteudo_artigo(session, noticia["link"]))

            resultados = await asyncio.gather(*tasks, return_exceptions=True)

            # Processar resultados
            for j, resultado in enumerate(resultados):
                if isinstance(resultado, Exception):
                    print(f"‚ùå Erro: {resultado}")
                    continue

                if not isinstance(resultado, tuple) or len(resultado) != 2:
                    print(f"‚ö†Ô∏è Resultado inv√°lido para √≠ndice {j}")
                    continue

                conteudo, titulo = resultado
                noticia_original = lote[j]

                # Atualizar dados
                noticia_corrigida = {
                    "data_publicacao": noticia_original["data_publicacao"],
                    "titulo": titulo,
                    "link": noticia_original["link"],
                    "texto_completo": (
                        conteudo if conteudo else noticia_original["texto_completo"]
                    ),
                }

                noticias_corrigidas.append(noticia_corrigida)

                # Mostrar progresso
                if titulo != "T√≠tulo n√£o encontrado":
                    print(f"‚úÖ T√≠tulo corrigido: {titulo[:80]}...")
                else:
                    print(
                        f"‚ö†Ô∏è T√≠tulo ainda n√£o encontrado para: {noticia_original['link']}"
                    )

            # Pausa entre lotes
            await asyncio.sleep(2)

    # Salvar CSV corrigido
    nome_arquivo = (
        f'noticias_amb_corrigidas_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
    )

    with open(nome_arquivo, "w", newline="", encoding="utf-8") as arquivo:
        campos = ["data_publicacao", "titulo", "link", "texto_completo"]
        escritor = csv.DictWriter(arquivo, fieldnames=campos)
        escritor.writeheader()
        escritor.writerows(noticias_corrigidas)

    # Estat√≠sticas finais
    titulos_corrigidos = sum(
        1 for n in noticias_corrigidas if n["titulo"] != "T√≠tulo n√£o encontrado"
    )

    print(f"\nüìä Corre√ß√£o Conclu√≠da!")
    print(f"‚úÖ T√≠tulos corrigidos: {titulos_corrigidos}/{len(noticias_corrigidas)}")
    print(f"üíæ Arquivo salvo: {nome_arquivo}")
    print(f"üìÑ Tamanho: {len(noticias_corrigidas)} not√≠cias")


if __name__ == "__main__":
    asyncio.run(corrigir_csv_noticias())
